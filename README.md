# üöÄ D√©ploiement RKE2 avec WireGuard - Scripts d'Automatisation

Ce repository contient tous les scripts n√©cessaires pour d√©ployer automatiquement un cluster RKE2 s√©curis√© avec WireGuard.

## üìÅ Fichiers Disponibles

```
.
‚îú‚îÄ‚îÄ README.md                           # Ce fichier
‚îú‚îÄ‚îÄ guide-rke2-wireguard-deployment.md  # Guide complet d√©taill√©
‚îú‚îÄ‚îÄ setup-loadbalancer.sh               # Configuration du Load Balancer
‚îú‚îÄ‚îÄ setup-rke2-node.sh                  # Installation RKE2 (master/worker)
‚îú‚îÄ‚îÄ generate-wireguard-client.sh        # G√©n√©ration config WireGuard
‚îú‚îÄ‚îÄ install-traefik.sh                  # Installation Traefik Ingress
‚îî‚îÄ‚îÄ verify-infrastructure.sh            # V√©rification de l'installation
```

## üìã Pr√©requis

- **OS**: Ubuntu 20.04/22.04/24.04 ou Debian 11/12
- **Acc√®s root** sur tous les serveurs
- **IP publique** pour le Load Balancer
- **Ressources minimum par n≈ìud**:
  - Master: 2 vCPU, 4 GB RAM, 50 GB disque
  - Worker: 2 vCPU, 4 GB RAM, 50 GB disque
  - Load Balancer: 1 vCPU, 2 GB RAM, 20 GB disque

## üö¶ Ordre d'Installation (IMPORTANT)

### √âtape 1Ô∏è‚É£ : Configuration du Load Balancer

**Sur le VPS Load Balancer (185.x.x.x) :**

```bash
# 1. Copier le script
wget https://github.com/OpsCatalog/archi-k8s/setup-loadbalancer.sh

# 2. √âditer les variables si n√©cessaire
nano setup-loadbalancer.sh
# Modifier : PUBLIC_IP="185.x.x.x"

# 3. Ex√©cuter le script
chmod +x setup-loadbalancer.sh
sudo ./setup-loadbalancer.sh

# 4. IMPORTANT: Noter la cl√© publique du serveur affich√©e
# Exemple: ServerPublicKey = AbCd1234EfGh5678...
```

**‚úÖ √Ä la fin de cette √©tape, vous devez avoir :**
- ‚úì WireGuard actif sur le Load Balancer
- ‚úì HAProxy configur√©
- ‚úì Cl√© publique du serveur not√©e
- ‚úì Firewall configur√©

## üìê Architecture HA

```
                        üåç Internet (185.x.x.x)
                               ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Load Balancer (LB) ‚îÇ
                    ‚îÇ  ‚Ä¢ HAProxy           ‚îÇ
                    ‚îÇ  ‚Ä¢ WireGuard         ‚îÇ
                    ‚îÇ  ‚Ä¢ 10.10.0.1         ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ                   ‚îÇ                   ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  master01   ‚îÇ     ‚îÇ  master02   ‚îÇ    ‚îÇ  worker01   ‚îÇ
    ‚îÇ  10.10.0.2  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  10.10.0.3  ‚îÇ    ‚îÇ  10.10.0.4  ‚îÇ
    ‚îÇ  RKE2 HA    ‚îÇetcd ‚îÇ  RKE2 HA    ‚îÇ    ‚îÇ  RKE2 Agent ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                   ‚îÇ
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                            ‚îÇ  worker02   ‚îÇ
                                            ‚îÇ  10.10.0.5  ‚îÇ
                                            ‚îÇ  RKE2 Agent ‚îÇ
                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîë Avantages de la Configuration HA

‚úÖ **Haute disponibilit√© du Control Plane** - Si un master tombe, l'autre prend le relai
‚úÖ **etcd distribu√©** - Base de donn√©es Kubernetes r√©pliqu√©e sur 2 n≈ìuds
‚úÖ **Load balancing automatique** - HAProxy distribue le trafic entre les masters
‚úÖ **Zero downtime** - Maintenance possible sans interruption de service

## üìã Plan d'Adressage

| Serveur | IP WireGuard | R√¥le | Services |
|---------|-------------|------|----------|
| loadbalancer | 10.10.0.1 | Load Balancer | HAProxy, WireGuard |
| master01 | 10.10.0.2 | Master HA #1 | RKE2 Server, etcd |
| master02 | 10.10.0.3 | Master HA #2 | RKE2 Server, etcd |
| worker01 | 10.10.0.4 | Worker | RKE2 Agent, Ingress |
| worker02 | 10.10.0.5 | Worker | RKE2 Agent, Ingress |

### ‚ö° √âtape 0 : Configurer HAProxy pour 2 Masters

**Sur le Load Balancer :**

```bash
# Appliquer la configuration HAProxy
chmod +x configure-haproxy-ha.sh
sudo ./configure-haproxy-ha.sh

# V√©rifier que HAProxy est bien configur√©
sudo systemctl status haproxy

# V√©rifier les ports
sudo netstat -tlnp | grep haproxy
```

**‚úÖ Vous devez voir les ports : 80, 443, 6443, 8404, 9345**

---

### 1Ô∏è‚É£ √âtape 1 : Configuration du PREMIER Master (master01)

**Sur master01 (qui deviendra 10.10.0.2) :**

```bash
# 1. Copier le script
scp setup-rke2-node.sh root@master01:/root/

# 2. Se connecter
ssh root@master01

# 3. Lancer l'installation
chmod +x setup-rke2-node.sh
./setup-rke2-node.sh master 10.10.0.2 10.10.0.1

# 4. Suivre les instructions :
#    - Entrer la cl√© publique du Load Balancer
#    - G√©n√©rer un nouveau token (r√©pondre 'y')
#    - IMPORTANT : NOTER LE TOKEN (n√©cessaire pour master02 ET les workers)
```

**‚è±Ô∏è Temps : ~5 minutes**

**Configuration appliqu√©e sur master01 :**
```yaml
# /etc/rancher/rke2/config.yaml
node-ip: 10.10.0.2
advertise-address: 10.10.0.2
tls-san:
  - 185.x.x.x
  - 10.10.0.1
  - 10.10.0.2
  - 10.10.0.3
  - master01
  - master02
token: "VotreTokenSecurise123456789"
cluster-cidr: 10.42.0.0/16
service-cidr: 10.43.0.0/16
cni:
  - calico
```

**V√©rifier que master01 est pr√™t :**

```bash
# Sur master01
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
export PATH=$PATH:/var/lib/rancher/rke2/bin

# Attendre que le n≈ìud soit Ready (peut prendre 2-3 minutes)
kubectl get nodes

# Devrait afficher :
# NAME       STATUS   ROLES                       AGE   VERSION
# master01   Ready    control-plane,etcd,master   3m    v1.28.x+rke2r1
```

---

### 2Ô∏è‚É£ √âtape 2 : Ajouter le Peer master01 sur le Load Balancer

**Retour sur le Load Balancer :**

```bash
# Ajouter master01 au r√©seau WireGuard
sudo add-wireguard-peer.sh master01 10.10.0.2

# Entrer la cl√© publique de master01 (affich√©e lors de son installation)

# V√©rifier la connectivit√©
ping 10.10.0.2

# Tester l'API Kubernetes
nc -zv 10.10.0.2 6443
```

**‚úÖ Les deux commandes doivent r√©ussir**

---

### 3Ô∏è‚É£ √âtape 3 : Configuration du SECOND Master (master02)

**‚ö†Ô∏è IMPORTANT : Attendre que master01 soit compl√®tement op√©rationnel avant de continuer !**

**Sur master02 (qui deviendra 10.10.0.3) :**

```bash
# 1. Copier le script
scp setup-rke2-node.sh root@master02:/root/

# 2. Se connecter
ssh root@master02

# 3. Cr√©er la configuration WireGuard MANUELLEMENT d'abord
apt update && apt install -y wireguard wireguard-tools

# G√©n√©rer les cl√©s
wg genkey | tee /etc/wireguard/private.key | wg pubkey > /etc/wireguard/public.key
chmod 600 /etc/wireguard/private.key

# Noter la cl√© publique
cat /etc/wireguard/public.key

# Cr√©er la config WireGuard
cat > /etc/wireguard/wg0.conf << EOF
[Interface]
PrivateKey = $(cat /etc/wireguard/private.key)
Address = 10.10.0.3/24

[Peer]
PublicKey = <CL√â_PUBLIQUE_DU_LOAD_BALANCER>
Endpoint = 185.x.x.x:51820
AllowedIPs = 10.10.0.0/24
PersistentKeepalive = 25
EOF

# D√©marrer WireGuard
systemctl enable wg-quick@wg0
systemctl start wg-quick@wg0

# V√©rifier la connectivit√©
ping 10.10.0.1
ping 10.10.0.2
```

**4. Ajouter master02 au Load Balancer**

**Sur le Load Balancer :**

```bash
sudo add-wireguard-peer.sh master02 10.10.0.3
# Entrer la cl√© publique de master02
```

**5. Installer RKE2 sur master02 (rejoint le cluster)**

**Sur master02 :**

```bash
# T√©l√©charger RKE2 Server
curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=server sh -

# Cr√©er la configuration - IMPORTANT: il rejoint master01
mkdir -p /etc/rancher/rke2

cat > /etc/rancher/rke2/config.yaml << EOF
# Rejoindre le cluster existant via le Load Balancer
server: https://10.10.0.1:9345

# LE M√äME TOKEN que master01
token: "VotreTokenSecurise123456789"

# Configuration du n≈ìud
node-ip: 10.10.0.3
advertise-address: 10.10.0.3

# TLS SANs
tls-san:
  - 185.x.x.x
  - 10.10.0.1
  - 10.10.0.2
  - 10.10.0.3
  - master01
  - master02

# Configuration r√©seau (doit √™tre identique √† master01)
cluster-cidr: 10.42.0.0/16
service-cidr: 10.43.0.0/16

# CNI
cni:
  - calico

# D√©sactiver l'ingress par d√©faut
disable:
  - rke2-ingress-nginx

# Permissions
write-kubeconfig-mode: "0644"
EOF

# D√©marrer RKE2 Server
systemctl enable rke2-server.service
systemctl start rke2-server.service

# Suivre les logs (peut prendre 3-5 minutes pour rejoindre le cluster)
journalctl -u rke2-server -f
```

**6. V√©rifier que master02 a rejoint le cluster**

**Sur master01 ou master02 :**

```bash
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
export PATH=$PATH:/var/lib/rancher/rke2/bin

kubectl get nodes

# Devrait maintenant afficher LES DEUX masters :
# NAME       STATUS   ROLES                       AGE   VERSION
# master01   Ready    control-plane,etcd,master   10m   v1.28.x+rke2r1
# master02   Ready    control-plane,etcd,master   3m    v1.28.x+rke2r1

# V√©rifier etcd (doit avoir 2 membres)
kubectl get nodes -o wide
kubectl get pods -n kube-system | grep etcd
```

**‚úÖ Vous avez maintenant un Control Plane HA avec etcd distribu√© !**

---

### 4Ô∏è‚É£ √âtape 4 : Configuration des Workers

**Sur worker01 (10.10.0.4) :**

```bash
scp setup-rke2-node.sh root@worker01:/root/
ssh root@worker01

chmod +x setup-rke2-node.sh
./setup-rke2-node.sh worker 10.10.0.4 10.10.0.1

# Entrer :
# - Cl√© publique du Load Balancer
# - LE M√äME TOKEN que les masters
# - IP du serveur : 10.10.0.1 (le Load Balancer - pas un master directement!)
```

**Ajouter le peer sur le Load Balancer :**

```bash
# Sur le LB
sudo add-wireguard-peer.sh worker01 10.10.0.4
```

**Sur worker02 (10.10.0.5) :**

```bash
scp setup-rke2-node.sh root@worker02:/root/
ssh root@worker02

chmod +x setup-rke2-node.sh
./setup-rke2-node.sh worker 10.10.0.5 10.10.0.1

# M√™me processus que worker01
```

**Ajouter le peer sur le Load Balancer :**

```bash
# Sur le LB
sudo add-wireguard-peer.sh worker02 10.10.0.5
```

**‚ö†Ô∏è IMPORTANT pour les workers :**
Les workers se connectent via le Load Balancer (`10.10.0.1:9345`), pas directement aux masters. HAProxy distribue automatiquement entre master01 et master02.

---

### 5Ô∏è‚É£ √âtape 5 : V√©rification Compl√®te

**Sur master01 :**

```bash
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
export PATH=$PATH:/var/lib/rancher/rke2/bin

# Tous les n≈ìuds doivent √™tre Ready
kubectl get nodes -o wide

# R√©sultat attendu :
# NAME       STATUS   ROLES                       AGE   VERSION        INTERNAL-IP
# master01   Ready    control-plane,etcd,master   15m   v1.28.x+rke2   10.10.0.2
# master02   Ready    control-plane,etcd,master   8m    v1.28.x+rke2   10.10.0.3
# worker01   Ready    worker                      5m    v1.28.x+rke2   10.10.0.4
# worker02   Ready    worker                      3m    v1.28.x+rke2   10.10.0.5

# V√©rifier les pods syst√®me
kubectl get pods -A

# V√©rifier etcd HA
kubectl get pods -n kube-system | grep etcd
# Doit montrer etcd sur master01 ET master02

# V√©rifier les endpoints de l'API
kubectl get endpoints kubernetes -n default
# Doit montrer master01:6443 ET master02:6443
```

**Sur le Load Balancer :**

```bash
# V√©rifier HAProxy Stats
curl http://localhost:8404/stats

# Ou dans un navigateur :
# http://185.x.x.x:8404/stats
# admin / ChangeMe123!

# V√©rifier que les 2 masters sont UP
```

---

### 6Ô∏è‚É£ √âtape 6 : Installation de Traefik

**Sur master01 :**

```bash
chmod +x install-traefik.sh
./install-traefik.sh

# V√©rifier
kubectl get pods -n traefik
kubectl get svc -n traefik
kubectl get pods -n test-app
```

---

### 7Ô∏è‚É£ √âtape 7 : Test de Haute Disponibilit√©

**Test 1 : Arr√™ter master01**

```bash
# Sur master01
sudo systemctl stop rke2-server

# Sur master02 (ou worker)
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
export PATH=$PATH:/var/lib/rancher/rke2/bin

kubectl get nodes
# L'API Kubernetes doit TOUJOURS fonctionner via master02

# Sur le Load Balancer, v√©rifier les logs HAProxy
journalctl -u haproxy -f
# Doit montrer que master01 est DOWN et le trafic va vers master02
```

**Test 2 : Red√©marrer master01**

```bash
# Sur master01
sudo systemctl start rke2-server

# Attendre 2 minutes

# V√©rifier qu'il rejoint le cluster
kubectl get nodes
# master01 doit revenir en Ready

# HAProxy doit automatiquement re-distribuer le trafic
```

---

## üéØ Configuration Sp√©ciale pour Workers

Les workers doivent se connecter via le Load Balancer, pas directement aux masters.

**Configuration worker (/etc/rancher/rke2/config.yaml) :**

```yaml
# SE CONNECTE VIA LE LOAD BALANCER
server: https://10.10.0.1:9345

# Token partag√©
token: "VotreTokenSecurise123456789"

# IP du worker
node-ip: 10.10.0.4  # ou 10.10.0.5 pour worker02

# Labels
node-label:
  - "node-role.kubernetes.io/worker=true"
  - "workload=application"
```

**Pourquoi via le Load Balancer ?**
- ‚úÖ Haute disponibilit√© automatique
- ‚úÖ Si un master tombe, les workers continuent via l'autre
- ‚úÖ Load balancing automatique du trafic d'enregistrement

---

## üîç Commandes de Diagnostic HA

```bash
# V√©rifier l'√©tat des masters
kubectl get nodes -l node-role.kubernetes.io/master

# V√©rifier etcd sur les deux masters
kubectl get pods -n kube-system -o wide | grep etcd

# Voir les membres etcd
kubectl exec -n kube-system etcd-master01 -- etcdctl member list

# V√©rifier les endpoints Kubernetes API
kubectl get endpoints kubernetes -n default

# HAProxy : voir les backends actifs
echo "show stat" | socat stdio /run/haproxy/admin.sock | grep k8s_api_backend

# Tester l'API via le Load Balancer
curl -k https://10.10.0.1:6443/healthz
```

---

## üõ°Ô∏è Avantages de cette Configuration

1. **Tol√©rance aux pannes** : Un master peut tomber sans interruption
2. **Maintenance sans downtime** : Mise √† jour des masters l'un apr√®s l'autre
3. **Performance** : Charge distribu√©e entre les masters
4. **√âvolutivit√©** : Facile d'ajouter un 3√®me master si n√©cessaire
5. **Production-ready** : Configuration recommand√©e par Rancher

---

## üìä R√©sum√© de l'Architecture

```
R√©seau WireGuard : 10.10.0.0/24
‚îú‚îÄ 10.10.0.1 : Load Balancer (HAProxy)
‚îÇ  ‚îú‚îÄ Port 6443  ‚Üí master01 + master02 (K8s API)
‚îÇ  ‚îú‚îÄ Port 9345  ‚Üí master01 + master02 (RKE2 Registration)
‚îÇ  ‚îú‚îÄ Port 80    ‚Üí worker01 + worker02 (HTTP)
‚îÇ  ‚îî‚îÄ Port 443   ‚Üí worker01 + worker02 (HTTPS)
‚îÇ
‚îú‚îÄ 10.10.0.2 : master01 (RKE2 Server + etcd)
‚îú‚îÄ 10.10.0.3 : master02 (RKE2 Server + etcd)
‚îú‚îÄ 10.10.0.4 : worker01 (RKE2 Agent + Ingress)
‚îî‚îÄ 10.10.0.5 : worker02 (RKE2 Agent + Ingress)
```

---

## ‚úÖ Checklist Finale

- [ ] Load Balancer : HAProxy configur√© pour 2 masters
- [ ] Load Balancer : WireGuard actif
- [ ] master01 : RKE2 Server install√© et Ready
- [ ] master01 : Peer ajout√© sur le LB
- [ ] master02 : RKE2 Server install√© et Ready (rejoint master01)
- [ ] master02 : Peer ajout√© sur le LB
- [ ] etcd : 2 membres actifs
- [ ] worker01 : RKE2 Agent install√© et Ready
- [ ] worker01 : Peer ajout√© sur le LB
- [ ] worker02 : RKE2 Agent install√© et Ready
- [ ] worker02 : Peer ajout√© sur le LB
- [ ] Traefik : Install√© et fonctionnel
- [ ] Test HA : Arr√™t/red√©marrage d'un master sans impact
- [ ] HAProxy Stats : Les 2 masters sont UP

Vous avez maintenant un cluster Kubernetes **production-ready** avec haute disponibilit√© ! üéâüöÄ


### √âtape 6Ô∏è‚É£ : V√©rifier le Cluster

**Sur le master01 :**

```bash
# V√©rifier que tous les n≈ìuds sont Ready
kubectl get nodes -o wide

# Devrait afficher :
# NAME       STATUS   ROLES                       AGE   VERSION
# master01   Ready    control-plane,etcd,master   10m   v1.28.x+rke2r1
# worker01   Ready    worker                      5m    v1.28.x+rke2r1
# worker02   Ready    worker                      5m    v1.28.x+rke2r1

# V√©rifier les pods syst√®me
kubectl get pods -A

# Tous les pods doivent √™tre en Running
```

### √âtape 7Ô∏è‚É£ : Installer Traefik Ingress

**Sur le master01 :**

```bash
# 1. Copier le script
scp install-traefik.sh root@master01:/root/

# 2. Ex√©cuter
chmod +x install-traefik.sh
./install-traefik.sh

# 3. V√©rifier l'installation
kubectl get pods -n traefik
kubectl get svc -n traefik

# 4. Tester l'application whoami
kubectl get pods -n test-app
```

**‚è±Ô∏è Temps d'installation : ~2 minutes**

### √âtape 8Ô∏è‚É£ : Test de Bout en Bout

**Sur le Load Balancer :**

```bash
# Tester l'application whoami via HAProxy
curl -H "Host: whoami.local" http://localhost

# Devrait retourner des informations sur le conteneur
```

**Depuis Internet :**

```bash
# Tester via l'IP publique du LoadBalancer
curl -H "Host: whoami.local" http://185.x.x.x
```

### √âtape 9Ô∏è‚É£ : V√©rification Compl√®te

**Sur n'importe quel n≈ìud :**

```bash
# Copier le script de v√©rification
scp verify-infrastructure.sh root@master01:/root/

# Ex√©cuter
chmod +x verify-infrastructure.sh
./verify-infrastructure.sh

# Devrait afficher un rapport complet avec :
# ‚úì Tous les services actifs
# ‚úì Connectivit√© r√©seau OK
# ‚úì Cluster Kubernetes op√©rationnel
```

## üîß Configuration Additionnelle

### Configuration du n≈ìud Infrastructure (optionnel)

**Sur infra-node (10.10.0.10) :**

```bash
# 1. Installer WireGuard
apt install -y wireguard

# 2. G√©n√©rer les cl√©s
wg genkey | tee /etc/wireguard/private.key | wg pubkey > /etc/wireguard/public.key
chmod 600 /etc/wireguard/private.key

# 3. Cr√©er la configuration
cat > /etc/wireguard/wg0.conf << EOF
[Interface]
PrivateKey = $(cat /etc/wireguard/private.key)
Address = 10.10.0.10/24

[Peer]
PublicKey = <CL√â_PUBLIQUE_DU_LB>
Endpoint = 185.x.x.x:51820
AllowedIPs = 10.10.0.0/24
PersistentKeepalive = 25
EOF

# 4. D√©marrer WireGuard
systemctl enable wg-quick@wg0
systemctl start wg-quick@wg0

# 5. Ajouter le peer sur le Load Balancer
# (sur le LB) sudo add-wireguard-peer.sh infra-node 10.10.0.10
```

### Installation de services sur le n≈ìud infra

```bash
# Installer Docker
curl -fsSL https://get.docker.com | sh

# Installer Docker Compose
curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose

# Exemple : MinIO
mkdir -p /opt/minio
cd /opt/minio

cat > docker-compose.yml << EOF
version: '3.8'
services:
  minio:
    image: minio/minio:latest
    restart: always
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: VotreMotDePasseSecurise
    volumes:
      - ./data:/data
    command: server /data --console-address ":9001"
EOF

docker-compose up -d
```

## üìä Monitoring et Logs

### Logs des services

```bash
# Sur le Load Balancer
journalctl -u wg-quick@wg0 -f
journalctl -u haproxy -f

# Sur le master
journalctl -u rke2-server -f

# Sur les workers
journalctl -u rke2-agent -f

# Logs Kubernetes
kubectl logs -f -n kube-system <pod-name>
kubectl logs -f -n traefik <traefik-pod>
```

### HAProxy Stats

Acc√©der √† : `http://185.x.x.x:8404/stats`
- Username: `admin`
- Password: `ChangeMe123!` (√† changer !)

### Traefik Dashboard

```bash
# Port-forward depuis le master
kubectl port-forward -n traefik $(kubectl get pods -n traefik -l app.kubernetes.io/name=traefik -o name) 9000:9000

# Acc√©der √† http://localhost:9000/dashboard/
```

## üõ†Ô∏è Commandes Utiles

### WireGuard

```bash
# Statut
wg show

# Red√©marrer
systemctl restart wg-quick@wg0

# Logs
journalctl -u wg-quick@wg0 -f
```

### RKE2

```bash
# N≈ìuds
kubectl get nodes
kubectl describe node <node-name>

# Pods
kubectl get pods -A
kubectl logs -f <pod-name> -n <namespace>

# Services
kubectl get svc -A

# Ingress
kubectl get ingress -A
```

### HAProxy

```bash
# Tester la config
haproxy -c -f /etc/haproxy/haproxy.cfg

# Red√©marrer
systemctl restart haproxy

# Logs
journalctl -u haproxy -f
```

## üîí S√©curit√©

### Changements recommand√©s

1. **Changer le mot de passe HAProxy**
```bash
nano /etc/haproxy/haproxy.cfg
# Modifier la ligne : stats auth admin:NouveauMotDePasse
systemctl restart haproxy
```

2. **Utiliser des certificats SSL**
```bash
# Installer cert-manager
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml
```

3. **Limiter l'acc√®s SSH**
```bash
# Configurer l'acc√®s par cl√© uniquement
nano /etc/ssh/sshd_config
# PasswordAuthentication no
# PermitRootLogin prohibit-password
systemctl restart sshd
```

4. **Activer les mises √† jour automatiques**
```bash
apt install unattended-upgrades
dpkg-reconfigure --priority=low unattended-upgrades
```

## üêõ D√©pannage

### Worker ne rejoint pas le cluster

```bash
# 1. V√©rifier la connectivit√© VPN
ping 10.10.0.2

# 2. V√©rifier que le port 9345 est accessible
nc -zv 10.10.0.2 9345

# 3. V√©rifier le token
cat /etc/rancher/rke2/config.yaml

# 4. Consulter les logs
journalctl -u rke2-agent -f
```

### HAProxy ne route pas le trafic

```bash
# 1. V√©rifier qu'HAProxy √©coute
netstat -tlnp | grep haproxy

# 2. Tester les backends
curl -v http://10.10.0.3:80

# 3. V√©rifier les logs
journalctl -u haproxy -f

# 4. Tester la config
haproxy -c -f /etc/haproxy/haproxy.cfg
```

### WireGuard ne se connecte pas

```bash
# 1. V√©rifier le service
systemctl status wg-quick@wg0

# 2. V√©rifier la config
wg show

# 3. V√©rifier le firewall
ufw status

# 4. Red√©marrer
systemctl restart wg-quick@wg0
```

## üìö Documentation

- [Guide complet d√©taill√©](./guide-rke2-wireguard-deployment.md)
- [RKE2 Documentation](https://docs.rke2.io/)
- [WireGuard Documentation](https://www.wireguard.com/)
- [HAProxy Documentation](https://www.haproxy.org/)
- [Traefik Documentation](https://doc.traefik.io/traefik/)

## üìû Support

Pour toute question ou probl√®me :
1. Consultez le guide d√©taill√©
2. Ex√©cutez le script de v√©rification
3. Consultez les logs des services
4. V√©rifiez la connectivit√© r√©seau

## ‚úÖ Checklist de Production

Avant de passer en production, v√©rifiez :

- [ ] Toutes les IPs sont correctement configur√©es
      
- [ ] Tous les n≈ìuds sont en √©tat "Ready"
      
- [ ] WireGuard fonctionne entre tous les n≈ìuds
      
- [ ] HAProxy route correctement le trafic
      
- [ ] Traefik r√©pond aux requ√™tes HTTP/HTTPS
      
- [ ] Les certificats SSL sont configur√©s
      
- [ ] Les mots de passe par d√©faut sont chang√©s
      
- [ ] Le firewall est correctement configur√©
      
- [ ] Les sauvegardes etcd sont configur√©es
      
- [ ] Le monitoring est en place
    
- [ ] La documentation est √† jour

## üéâ F√©licitations !

Si vous √™tes arriv√© jusqu'ici et que tous les tests passent, vous avez maintenant :

‚úÖ Un cluster Kubernetes RKE2 fonctionnel
‚úÖ Un r√©seau priv√© s√©curis√© avec WireGuard
‚úÖ Un Load Balancer avec haute disponibilit√©
‚úÖ Un Ingress Controller pr√™t pour vos applications
‚úÖ Une infrastructure scalable et s√©curis√©e

**Prochaines √©tapes recommand√©es :**
1. D√©ployer vos applications
2. Configurer le monitoring avanc√©
3. Mettre en place les sauvegardes
4. Configurer la CI/CD
5. Documenter vos proc√©dures

Bon d√©ploiement ! üöÄ
